{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile normalization with NumPy and SciPy\n",
    "\n",
    "> Distress not yourself if you cannot at first understand the deeper mysteries\n",
    "> of Spaceland. By degrees they will dawn upon you.\n",
    ">\n",
    "> â€” Edwin A. Abbott, *Flatland: A Romance of Many Dimensions*\n",
    "\n",
    "In this chapter, we will continue to analyze the gene expression data from Chapter 1, but with a slightly different purpose: __we want to use each patient's *gene expression profile* (the full vector of their gene expression measurements) to predict their expected survival__.\n",
    "In order to use full profiles, we need a stronger normalization than what Chapter 1's RPKM provides.\n",
    "We will instead perform [*quantile normalization*](https://en.wikipedia.org/wiki/Quantile_normalization), a technique that ensures measurements fit a specific distribution.\n",
    "This method enforces a strong assumption: if the data are not distributed according to a desired shape, we just make it fit!\n",
    "This might feel a bit like cheating, but it turns out to be __simple and useful in many cases where the specific distribution doesn't matter__, but the __relative changes of values within a population are important__.\n",
    "For example, Bolstad and colleagues [showed](https://doi.org/10.1093/bioinformatics/19.2.185) that it performs admirably in recovering known expression levels in microarray data.\n",
    "\n",
    "Over the course of the chapter, we will reproduce a simplified version of [Figures 5A and 5B](http://www.cell.com/action/showImagesData?pii=S0092-8674%2815%2900634-0) from this [paper](http://dx.doi.org/10.1016/j.cell.2015.05.044), which comes from The Cancer Genome Atlas (TCGA) project.\n",
    "\n",
    "Our implementation of quantile normalization uses NumPy and SciPy effectively to produce a function that is fast, efficient, and elegant. \n",
    "\n",
    "__Quantile normalization involves three steps__:\n",
    "- sort the values along each column,\n",
    "- find the average of each resulting row, and\n",
    "- replace each column quantile with the quantile of the average column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def quantile_norm(X):\n",
    "    \"\"\"Normalize the columns of X to each have the same distribution.\n",
    "\n",
    "    Given an expression matrix (microarray data, read counts, etc) of M genes\n",
    "    by N samples, quantile normalization ensures all samples have the same\n",
    "    spread of data (by construction).\n",
    "\n",
    "    The data across each row are averaged to obtain an average column. Each\n",
    "    column quantile is replaced with the corresponding quantile of the average\n",
    "    column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array of float, shape (M, N)\n",
    "        The input data, with M rows (genes/features) and N columns (samples).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xn : 2D array of float, shape (M, N)\n",
    "        The normalized data.\n",
    "    \"\"\"\n",
    "    # compute the quantiles\n",
    "    quantiles = np.mean(np.sort(X, axis=0), axis=1)\n",
    "\n",
    "    # compute the column-wise ranks. Each observation is replaced with its\n",
    "    # rank in that column: the smallest observation is replaced by 1, the\n",
    "    # second-smallest by 2, ..., and the largest by M, the number of rows.\n",
    "    ranks = np.apply_along_axis(stats.rankdata, 0, X)\n",
    "\n",
    "    # convert ranks to integer indices from 0 to M-1\n",
    "    rank_indices = ranks.astype(int) - 1\n",
    "\n",
    "    # index the quantiles for each rank with the ranks matrix\n",
    "    Xn = quantiles[rank_indices]\n",
    "\n",
    "    return(Xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the kind of variability of gene expression count data, it is __common practice to log-transform the data before quantile-normalizing__.\n",
    "Thus, we write an additional helper function to transform to log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_norm_log(X):\n",
    "    logX = np.log(X + 1)\n",
    "    logXn = quantile_norm(logX)\n",
    "    return logXn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, these two functions illustrate many of the things that make NumPy powerful (you will remember the first three of these moves from chapter 1):\n",
    "\n",
    "1. Arrays can be one-dimensional, like lists, but they can also be two-dimensional, like matrices, and higher-dimensional still. This allows them to represent many different kinds of numerical data. In our case, we are representing a 2D matrix.\n",
    "2. Arrays allow the expression of many numerical operations at once. In the  first line of `quantile_norm_log`, we add one and take the logarithm for every value in `X` in a single call. This is called __*vectorization*__.\n",
    "3. Arrays can be operated on along *axes*. In the first line of `quantile_norm`, we sort the data along each column just by specifying an `axis` parameter to `np.sort`. We then take the mean along each row by specifying a *different* `axis`.\n",
    "4. Arrays underpin the scientific Python ecosystem. The `scipy.stats.rankdata` function operates not on Python lists, but on NumPy arrays. This is true of many scientific libraries in Python.\n",
    "5. Even functions that don't have an `axis=` keyword can be made to operate along axes by NumPy's `apply_along_axis` function.\n",
    "6. Arrays support many kinds of data manipulation through *fancy indexing*:\n",
    "`Xn = quantiles[ranks]`. This is possibly the trickiest part of NumPy, but\n",
    "also among the most useful. We will explore it further in the text that\n",
    "follows.\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "As in Chapter 1, we will be working with the The Cancer Genome Atlas (TCGA) skin cancer RNAseq data set.\n",
    "Our goal is to predict mortality in skin cancer patients using their RNA expression data.\n",
    "By the end of this chapter we will have reproduced a simplified version of [Figures 5A and 5B](http://www.cell.com/action/showImagesData?pii=S0092-8674%2815%2900634-0) of a [paper](http://dx.doi.org/10.1016/j.cell.2015.05.044) from the TCGA consortium.\n",
    "\n",
    "As in Chapter 1, first we will use Pandas to make our job of reading in the data much easier.\n",
    "First we will read in our counts data as a pandas table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import TCGA melanoma data\n",
    "filename = 'data/counts.txt'\n",
    "data_table = pd.read_csv(filename, index_col=0)  # Parse file with pandas\n",
    "\n",
    "print(data_table.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the rows and columns of `data_table`, we can see that the\n",
    "columns are the samples, and the rows are the genes.\n",
    "Now let's put our counts in a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D ndarray containing expression counts for each gene in each individual\n",
    "counts = data_table.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene expression distribution differences between individuals\n",
    "\n",
    "Now, let's get a feel for our counts data by plotting the distribution of counts for each individual.\n",
    "We will use a Gaussian kernel to smooth out bumps in our data so we can get a\n",
    "better idea of the overall shape.\n",
    "\n",
    "First, as usual, we set our plotting style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots appear inline, set custom plotting style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('style/elegant.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a plotting function that makes use of SciPy's `gaussian_kde` function to plot smooth distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def plot_col_density(data):\n",
    "    \"\"\"For each column, produce a density plot over all rows.\"\"\"\n",
    "\n",
    "    # Use Gaussian smoothing to estimate the density\n",
    "    density_per_col = [stats.gaussian_kde(col) for col in data.T]\n",
    "    x = np.linspace(np.min(data), np.max(data), 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for density in density_per_col:\n",
    "        ax.plot(x, density(x))\n",
    "    ax.set_xlabel('Data values (per column)')\n",
    "    ax.set_ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use that function to plot the distributions of the raw data,\n",
    "before we have done any normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalization\n",
    "log_counts = np.log(counts + 1)\n",
    "plot_col_density(log_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Density of gene expression counts for each individual (log scale)\" -->\n",
    "\n",
    "We can see that while the distributions of counts are broadly similar,\n",
    "some individuals have flatter distributions and a few are pushed right over to the left.\n",
    "In fact, realizing that this is a log scale, the location of the peak of the\n",
    "distributions actually varies over an order of magnitude!\n",
    "When doing our analysis of the counts data later in this chapter, we will be assuming\n",
    "that changes in gene expression are due to biological differences between our samples.\n",
    "But a major distribution shift like this suggests that the differences are technical.\n",
    "That is, the changes are likely due to differences in the way we processed each sample,\n",
    "rather than due to biological variation.\n",
    "So we will try to normalize out these global differences between individuals.\n",
    "\n",
    "To do this normalization, we will perform quantile normalization, as described\n",
    "at the start of the chapter.  The idea is that all our samples should have a\n",
    "similar distribution, so any differences in the shape should be due to some\n",
    "technical variation.  More formally, given an expression matrix (microarray\n",
    "data, read counts, etc) of shape `(n_genes, n_samples)`, quantile normalization\n",
    "ensures that all samples (columns) have the same spread of data by construction.\n",
    "\n",
    "With NumPy and SciPy, this can be done easily and efficiently.\n",
    "To recap, here is our quantile normalization implementation, which we introduced at the beginning of the chapter.\n",
    "\n",
    "Let's assume we've read in the input matrix as X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def quantile_norm(X):\n",
    "    \"\"\"Normalize the columns of X to each have the same distribution.\n",
    "\n",
    "    Given an expression matrix (microarray data, read counts, etc) of M genes\n",
    "    by N samples, quantile normalization ensures all samples have the same\n",
    "    spread of data (by construction).\n",
    "\n",
    "    The data across each row are averaged to obtain an average column. Each\n",
    "    column quantile is replaced with the corresponding quantile of the average\n",
    "    column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array of float, shape (M, N)\n",
    "        The input data, with M rows (genes/features) and N columns (samples).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xn : 2D array of float, shape (M, N)\n",
    "        The normalized data.\n",
    "    \"\"\"\n",
    "    # compute the quantiles\n",
    "    quantiles = np.mean(np.sort(X, axis=0), axis=1)\n",
    "\n",
    "    # compute the column-wise ranks. Each observation is replaced with its\n",
    "    # rank in that column: the smallest observation is replaced by 1, the\n",
    "    # second-smallest by 2, ..., and the largest by M, the number of rows.\n",
    "    ranks = np.apply_along_axis(stats.rankdata, 0, X)\n",
    "\n",
    "    # convert ranks to integer indices from 0 to M-1\n",
    "    rank_indices = ranks.astype(int) - 1\n",
    "\n",
    "    # index the quantiles for each rank with the ranks matrix\n",
    "    Xn = quantiles[rank_indices]\n",
    "\n",
    "    return(Xn)\n",
    "\n",
    "\n",
    "def quantile_norm_log(X):\n",
    "    logX = np.log(X + 1)\n",
    "    logXn = quantile_norm(logX)\n",
    "    return logXn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what our distributions look like after quantile normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization\n",
    "log_counts_normalized = quantile_norm_log(counts)\n",
    "\n",
    "plot_col_density(log_counts_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Density of gene expression counts for each individual after quantile normalization (log scale)\" -->\n",
    "\n",
    "As you might expect, the distributions now look virtually identical!\n",
    "(The different left tails of the distribution have to do with different\n",
    "numbers of ties for low count values â€” 0, 1, 2, ... â€” in the different\n",
    "columns of the data.)\n",
    "\n",
    "Now that we have normalized our counts, we can start using our gene expression data to predict patient prognosis.\n",
    "\n",
    "## Biclustering the counts data\n",
    "\n",
    "\n",
    "Clustering the samples tells us which samples have similar gene expression profiles, which may indicate similar characteristics of the samples on other scales.\n",
    "Now that the data are normalized, we can cluster the genes (rows) and samples (columns) of the expression matrix.\n",
    "Clustering the rows tells us which genes' expression values are linked, which is an indication that they work together in the process being studied.\n",
    "*Biclustering* means that we are simultaneously clustering both the rows and columns of our data.\n",
    "By clustering along the rows we find out with genes are working together, and by clustering along the columns we find out which samples are similar.\n",
    "\n",
    "Because clustering can be an expensive operation, we will limit our analysis to the 1,500 genes that are most variable, since these will account for most of the correlation signal in either dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_variable_rows(data, *, n=1500):\n",
    "    \"\"\"Subset data to the n most variable rows\n",
    "\n",
    "    In this case, we want the n most variable genes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2D array of float\n",
    "        The data to be subset\n",
    "    n : int, optional\n",
    "        Number of rows to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    variable_data : 2D array of float\n",
    "        The `n` rows of `data` that exhibit the most variance.\n",
    "    \"\"\"\n",
    "    # compute variance along the columns axis\n",
    "    rowvar = np.var(data, axis=1)\n",
    "    # Get sorted indices (ascending order), take the last n\n",
    "    sort_indices = np.argsort(rowvar)[-n:]\n",
    "    # use as index for data\n",
    "    variable_data = data[sort_indices, :]\n",
    "    return variable_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function to bicluster the data.\n",
    "Normally, you would use a sophisticated clustering algorithm from the [scikit-learn](http://scikit-learn.org) library for this.\n",
    "In our case, we want to use __hierarchical clustering for simplicity and ease of display__.\n",
    "The SciPy library happens to have a perfectly good hierarchical clustering module, though it requires a bit of wrangling to get your head around its interface.\n",
    "\n",
    "As a reminder, hierarchical clustering is a method to group observations using sequential merging of clusters:\n",
    "initially, every observation is its own cluster.\n",
    "Then, the two nearest clusters are repeatedly merged, and then the next two,\n",
    "and so on, until every observation is in a single cluster.\n",
    "This sequence of merges forms a *merge tree*.\n",
    "By cutting the tree at a specific height, we can get a finer or coarser clustering of observations.\n",
    "\n",
    "The `linkage` function in `scipy.cluster.hierarchy` performs a hierarchical clustering of the rows of a matrix, using a particular metric (for example, Euclidean distance, Manhattan distance, or others) and a particular linkage method, the distance between two clusters (for example, the average distance between all the observations in a pair of clusters).\n",
    "\n",
    "It returns the merge tree as a \"linkage matrix\", which contains each merge operation along with the distance computed for the merge and the number of observations in the resulting cluster. From the `linkage` documentation:\n",
    "\n",
    "> A cluster with an index less than $n$ corresponds to one of\n",
    "> the $n$ original observations. The distance between\n",
    "> clusters `Z[i, 0]` and `Z[i, 1]` is given by `Z[i, 2]`. The\n",
    "> fourth value `Z[i, 3]` represents the number of original\n",
    "> observations in the newly formed cluster.\n",
    "\n",
    "Whew! That's a lot of information, but let's dive right in and hopefully you'll get the hang of it rather quickly.\n",
    "First, we define a function, `bicluster`, that clusters both the rows *and* the columns of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "\n",
    "def bicluster(data, linkage_method='average', distance_metric='correlation'):\n",
    "    \"\"\"Cluster the rows and the columns of a matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2D ndarray\n",
    "        The input data to bicluster.\n",
    "    linkage_method : string, optional\n",
    "        Method to be passed to `linkage`.\n",
    "    distance_metric : string, optional\n",
    "        Distance metric to use for clustering. See the documentation\n",
    "        for ``scipy.spatial.distance.pdist`` for valid metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_rows : linkage matrix\n",
    "        The clustering of the rows of the input data.\n",
    "    y_cols : linkage matrix\n",
    "        The clustering of the cols of the input data.\n",
    "    \"\"\"\n",
    "    y_rows = linkage(data, method=linkage_method, metric=distance_metric)\n",
    "    y_cols = linkage(data.T, method=linkage_method, metric=distance_metric)\n",
    "    return y_rows, y_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple: we just call `linkage` for the input matrix and also for the *transpose* of that matrix, in which columns become rows and rows become columns.\n",
    "\n",
    "## Visualizing clusters\n",
    "\n",
    "Next, we define a function to visualize the output of that clustering.\n",
    "We are going to rearrange the rows and columns of the input data so that similar rows are together and similar columns are together.\n",
    "And we are additionally going to show the merge tree for both rows and columns, displaying which observations belong together for each.\n",
    "The merge trees are presented as dendrograms, with the branch-lengths indicating how similar the observations are to each other (shorter = more similar).\n",
    "\n",
    "As a word of warning, there is a fair bit of hard-coding of parameters going on here.\n",
    "This is difficult to avoid for plotting, where design is often a matter of eyeballing to find the correct proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, leaves_list\n",
    "\n",
    "\n",
    "def clear_spines(axes):\n",
    "    for loc in ['left', 'right', 'top', 'bottom']:\n",
    "        axes.spines[loc].set_visible(False)\n",
    "    axes.set_xticks([])\n",
    "    axes.set_yticks([])\n",
    "\n",
    "\n",
    "def plot_bicluster(data, row_linkage, col_linkage,\n",
    "                   row_nclusters=10, col_nclusters=3):\n",
    "    \"\"\"Perform a biclustering, plot a heatmap with dendrograms on each axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array of float, shape (M, N)\n",
    "        The input data to bicluster.\n",
    "    row_linkage : array, shape (M-1, 4)\n",
    "        The linkage matrix for the rows of `data`.\n",
    "    col_linkage : array, shape (N-1, 4)\n",
    "        The linkage matrix for the columns of `data`.\n",
    "    n_clusters_r, n_clusters_c : int, optional\n",
    "        Number of clusters for rows and columns.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4.8, 4.8))\n",
    "\n",
    "    # Compute and plot row-wise dendrogram\n",
    "    # `add_axes` takes a \"rectangle\" input to add a subplot to a figure.\n",
    "    # The figure is considered to have side-length 1 on each side, and its\n",
    "    # bottom-left corner is at (0, 0).\n",
    "    # The measurements passed to `add_axes` are the left, bottom, width, and\n",
    "    # height of the subplot. Thus, to draw the left dendrogram (for the rows),\n",
    "    # we create a rectangle whose bottom-left corner is at (0.09, 0.1), and\n",
    "    # measuring 0.2 in width and 0.6 in height.\n",
    "    ax1 = fig.add_axes([0.09, 0.1, 0.2, 0.6])\n",
    "    # For a given number of clusters, we can obtain a cut of the linkage\n",
    "    # tree by looking at the corresponding distance annotation in the linkage\n",
    "    # matrix.\n",
    "    threshold_r = (row_linkage[-row_nclusters, 2] +\n",
    "                   row_linkage[-row_nclusters+1, 2]) / 2\n",
    "    with plt.rc_context({'lines.linewidth': 0.75}):\n",
    "        dendrogram(row_linkage, orientation='left',\n",
    "                   color_threshold=threshold_r, ax=ax1)\n",
    "    clear_spines(ax1)\n",
    "\n",
    "    # Compute and plot column-wise dendrogram\n",
    "    # See notes above for explanation of parameters to `add_axes`\n",
    "    ax2 = fig.add_axes([0.3, 0.71, 0.6, 0.2])\n",
    "    threshold_c = (col_linkage[-col_nclusters, 2] +\n",
    "                   col_linkage[-col_nclusters+1, 2]) / 2\n",
    "    with plt.rc_context({'lines.linewidth': 0.75}):\n",
    "        dendrogram(col_linkage, color_threshold=threshold_c, ax=ax2)\n",
    "    clear_spines(ax2)\n",
    "\n",
    "    # Plot data heatmap\n",
    "    ax = fig.add_axes([0.3, 0.1, 0.6, 0.6])\n",
    "\n",
    "    # Sort data by the dendrogram leaves\n",
    "    idx_rows = leaves_list(row_linkage)\n",
    "    data = data[idx_rows, :]\n",
    "    idx_cols = leaves_list(col_linkage)\n",
    "    data = data[:, idx_cols]\n",
    "\n",
    "    im = ax.imshow(data, aspect='auto', origin='lower', cmap='YlGnBu_r')\n",
    "    clear_spines(ax)\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel('Samples')\n",
    "    ax.set_ylabel('Genes', labelpad=125)\n",
    "\n",
    "    # Plot legend\n",
    "    axcolor = fig.add_axes([0.91, 0.1, 0.02, 0.6])\n",
    "    plt.colorbar(im, cax=axcolor)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply these functions to our normalized counts matrix to display row and column clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_log = np.log(counts + 1)\n",
    "counts_var = most_variable_rows(counts_log, n=1500)\n",
    "yr, yc = bicluster(counts_var, linkage_method='ward',\n",
    "                   distance_metric='euclidean')\n",
    "with plt.style.context('style/thinner.mplstyle'):\n",
    "    plot_bicluster(counts_var, yr, yc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"This heatmap shows the level of gene expression across all samples and genes. The color indicates the expression level. The rows and columns are grouped by our clusters. We can see our gene clusters along the y-axis and sample clusters across the top of the x-axis.\" -->\n",
    "\n",
    "## Predicting survival\n",
    "\n",
    "We can see that the sample data naturally falls into at least 2 clusters, maybe 3.\n",
    "Are these clusters meaningful?\n",
    "To answer this, we can access the patient data, available from the [data repository](https://tcga-data.nci.nih.gov/docs/publications/skcm_2015/) for the paper.\n",
    "After some preprocessing, we get the [patients table](https://github.com/elegant-scipy/elegant-scipy/blob/master/data/patients.csv), which contains survival information for each patient.\n",
    "We can then match these to the counts clusters, and understand whether the patients' gene expression can predict differences in their pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv('data/patients.csv', index_col=0)\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each patient (the rows) we have:\n",
    "\n",
    "- UVÂ­ signature: Ultraviolet light tends to cause specific DNA mutations.\n",
    "By looking for this mutation signature researchers can infer whether UV light likely caused the mutation(s) that lead to cancer in these patients.\n",
    "- originalÂ­ clusters: In the paper, the patients were clustered using gene expression data.\n",
    "These clusters were classified according to the types of genes that typified that cluster.\n",
    "The main clusters were \"immune\" (n = 168; 51%), \"keratin\" (n = 102; 31%), and \"MITF-low\" (n = 59; 18%).\n",
    "- melanomaÂ­ survivalÂ­ time: Number of days that the patient survived.\n",
    "- melanomaÂ­ dead: 1 if the patient died of melanoma, 0 if they are alive or died of something else.\n",
    "\n",
    "Now we need to draw *survival curves* for each group of patients defined by the clustering.\n",
    "This is a plot of the fraction of a population that remains alive over a period of time.\n",
    "Note that some data is *right-censored*, which means that in some cases, we don't actually know when the patient died, or the patient might have died of causes unrelated to the melanoma.\n",
    "We count these patients as \"alive\" for the duration of the survival curve, but more sophisticated analyses might try to estimate their likely time of death.\n",
    "\n",
    "To obtain a survival curve from survival times, we create a step function that decreases by $1/n$ at each step, where $n$ is the number of patients in the group.\n",
    "We then match that function against the non-censored survival times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_distribution_function(lifetimes, right_censored=None):\n",
    "    \"\"\"Return the survival distribution function of a set of lifetimes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lifetimes : array of float or int\n",
    "        The observed lifetimes of a population. These must be non-\n",
    "        -negative.\n",
    "    right_censored : array of bool, same shape as `lifetimes`\n",
    "        A value of `True` here indicates that this lifetime was not\n",
    "        observed. Values of `np.nan` in `lifetimes` are also considered\n",
    "        to be right-censored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_lifetimes : array of float\n",
    "        The\n",
    "    sdf : array of float\n",
    "        Values starting at 1 and progressively decreasing, one level\n",
    "        for each observation in `lifetimes`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    In this example, of a population of four, two die at time 1, a\n",
    "    third dies at time 2, and a final individual dies at an unknown\n",
    "    time. (Hence, ``np.nan``.)\n",
    "\n",
    "    >>> lifetimes = np.array([2, 1, 1, np.nan])\n",
    "    >>> survival_distribution_function(lifetimes)\n",
    "    (array([ 0.,  1.,  1.,  2.]), array([ 1.  ,  0.75,  0.5 ,  0.25]))\n",
    "    \"\"\"\n",
    "    n_obs = len(lifetimes)\n",
    "    rc = np.isnan(lifetimes)\n",
    "    if right_censored is not None:\n",
    "        rc |= right_censored\n",
    "    observed = lifetimes[~rc]\n",
    "    xs = np.concatenate( ([0], np.sort(observed)) )\n",
    "    ys = np.linspace(1, 0, n_obs + 1)\n",
    "    ys = ys[:len(xs)]\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can easily obtain survival curves from the survival data, we can plot them.\n",
    "We write a function that groups the survival times by cluster identity and plots each group as a different line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_survival_curves(clusters, sample_names, patients,\n",
    "                                 censor=True):\n",
    "    \"\"\"Plot the survival data from a set of sample clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clusters : array of int or categorical pd.Series\n",
    "        The cluster identity of each sample, encoded as a simple int\n",
    "        or as a pandas categorical variable.\n",
    "    sample_names : list of string\n",
    "        The name corresponding to each sample. Must be the same length\n",
    "        as `clusters`.\n",
    "    patients : pandas.DataFrame\n",
    "        The DataFrame containing survival information for each patient.\n",
    "        The indices of this DataFrame must correspond to the\n",
    "        `sample_names`. Samples not represented in this list will be\n",
    "        ignored.\n",
    "    censor : bool, optional\n",
    "        If `True`, use `patients['melanoma-dead']` to right-censor the\n",
    "        survival data.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    if type(clusters) == np.ndarray:\n",
    "        cluster_ids = np.unique(clusters)\n",
    "        cluster_names = ['cluster {}'.format(i) for i in cluster_ids]\n",
    "    elif type(clusters) == pd.Series:\n",
    "        cluster_ids = clusters.cat.categories\n",
    "        cluster_names = list(cluster_ids)\n",
    "    n_clusters = len(cluster_ids)\n",
    "    for c in cluster_ids:\n",
    "        clust_samples = np.flatnonzero(clusters == c)\n",
    "        # discard patients not present in survival data\n",
    "        clust_samples = [sample_names[i] for i in clust_samples\n",
    "                         if sample_names[i] in patients.index]\n",
    "        patient_cluster = patients.loc[clust_samples]\n",
    "        survival_times = patient_cluster['melanoma-survival-time'].values\n",
    "        if censor:\n",
    "            censored = ~patient_cluster['melanoma-dead'].values.astype(bool)\n",
    "        else:\n",
    "            censored = None\n",
    "        stimes, sfracs = survival_distribution_function(survival_times,\n",
    "                                                        censored)\n",
    "        ax.plot(stimes / 365, sfracs)\n",
    "\n",
    "    ax.set_xlabel('survival time (years)')\n",
    "    ax.set_ylabel('fraction alive')\n",
    "    ax.legend(cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `fcluster` function to obtain cluster identities for the samples (columns of the counts data), and plot each survival curve separately.\n",
    "The `fcluster` function takes a linkage matrix, as returned by `linkage`, and a threshold, and returns cluster identities.\n",
    "It's difficult to know a-priori what the threshold should be, but we can obtain the appropriate threshold for a fixed number of clusters by checking the distances in the linkage matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "n_clusters = 3\n",
    "threshold_distance = (yc[-n_clusters, 2] + yc[-n_clusters+1, 2]) / 2\n",
    "clusters = fcluster(yc, threshold_distance, 'distance')\n",
    "\n",
    "plot_cluster_survival_curves(clusters, data_table.columns, patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Survival curves for patients clustered using gene expression data\" -->\n",
    "\n",
    "The clustering of gene expression profiles appears to have identified a\n",
    "higher-risk subtype of melanoma (cluster 2).\n",
    "The TCGA study backs this claim up with a more robust clustering and\n",
    "statistical testing. This is indeed only the latest study to show such a\n",
    "result, with others identifying subtypes of leukemia (blood cancer), gut\n",
    "cancer, and more. Although the above clustering technique is quite fragile,\n",
    "there are other, more robust ways to explore this and similar datasets [^paper].\n",
    "\n",
    "<!-- exercise begin -->\n",
    "\n",
    "**Exercise:** Do our clusters do a better job of predicting survival than the original clusters in the paper? What about UV signature?\n",
    "\n",
    "Plot survival curves using the original clusters and UV signature columns of the patient data. How do they compare to our clusters?\n",
    "<!-- exercise end -->\n",
    "\n",
    "<!-- exercise begin -->\n",
    "\n",
    "**Exercise:** We leave you the exercise of implementing the approach described in the paper[^paper]:\n",
    "\n",
    "1. Take bootstrap samples (random choice with replacement) of the genes used to cluster the samples;\n",
    "2. For each sample, produce a hierarchical clustering;\n",
    "3. In a `(n_samples, n_samples)`-shaped matrix, store the number of times a sample pair appears together in a bootstrapped clustering.\n",
    "4. Perform a hierarchical clustering on the resulting matrix.\n",
    "\n",
    "This identifies groups of samples that frequently occur together in clusterings, regardless of the genes chosen.\n",
    "Thus, these samples can be considered to robustly cluster together.\n",
    "\n",
    "*Hint: use `np.random.choice` with `replacement=True` to create bootstrap samples of row indices.*\n",
    "\n",
    "<!-- exercise end -->\n",
    "\n",
    "[^paper]: The Cancer Genome Atlas Network. (2015) Genomic Classification of\n",
    "          Cutaneous Melanoma. Cell 161:1681-1696.\n",
    "          http://dx.doi.org/10.1016/j.cell.2015.05.044"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
